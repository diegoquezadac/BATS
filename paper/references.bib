@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{radford2022robust,
      title={Robust Speech Recognition via Large-Scale Weak Supervision}, 
      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
      year={2022},
      eprint={2212.04356},
      archivePrefix={arXiv},
      primaryClass={eess.AS}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}
@article{timit,
author = {Garofolo, J. and Lamel, Lori and Fisher, W. and Fiscus, Jonathan and Pallett, D. and Dahlgren, N. and Zue, V.},
year = {1992},
month = {11},
pages = {},
title = {TIMIT Acoustic-phonetic Continuous Speech Corpus},
journal = {Linguistic Data Consortium}
}
@inproceedings{commonvoice:2020,
  author = {Ardila, R. and Branson, M. and Davis, K. and Henretty, M. and Kohler, M. and Meyer, J. and Morais, R. and Saunders, L. and Tyers, F. M. and Weber, G.},
  title = {Common Voice: A Massively-Multilingual Speech Corpus},
  booktitle = {Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020)},
  pages = {4211--4215},
  year = 2020
}

@article{DBLP:journals/corr/abs-1912-06670,
  author       = {Rosana Ardila and
                  Megan Branson and
                  Kelly Davis and
                  Michael Henretty and
                  Michael Kohler and
                  Josh Meyer and
                  Reuben Morais and
                  Lindsay Saunders and
                  Francis M. Tyers and
                  Gregor Weber},
  title        = {Common Voice: {A} Massively-Multilingual Speech Corpus},
  journal      = {CoRR},
  volume       = {abs/1912.06670},
  year         = {2019},
  url          = {http://arxiv.org/abs/1912.06670},
  eprinttype    = {arXiv},
  eprint       = {1912.06670},
  timestamp    = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1912-06670.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/SelvarajuDVCPB16,
  author       = {Ramprasaath R. Selvaraju and
                  Abhishek Das and
                  Ramakrishna Vedantam and
                  Michael Cogswell and
                  Devi Parikh and
                  Dhruv Batra},
  title        = {Grad-CAM: Why did you say that? Visual Explanations from Deep Networks
                  via Gradient-based Localization},
  journal      = {CoRR},
  volume       = {abs/1610.02391},
  year         = {2016},
  url          = {http://arxiv.org/abs/1610.02391},
  eprinttype    = {arXiv},
  eprint       = {1610.02391},
  timestamp    = {Mon, 13 Aug 2018 16:46:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SelvarajuDVCPB16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{wu2023trust,
      title={Can We Trust Explainable AI Methods on ASR? An Evaluation on Phoneme Recognition}, 
      author={Xiaoliang Wu and Peter Bell and Ajitha Rajan},
      year={2023},
      eprint={2305.18011},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-2008-00582,
  author       = {Verena Haunschmid and
                  Ethan Manilow and
                  Gerhard Widmer},
  title        = {audioLIME: Listenable Explanations Using Source Separation},
  journal      = {CoRR},
  volume       = {abs/2008.00582},
  year         = {2020},
  url          = {https://arxiv.org/abs/2008.00582},
  eprinttype    = {arXiv},
  eprint       = {2008.00582},
  timestamp    = {Fri, 07 Aug 2020 15:07:21 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2008-00582.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ribeiro2016why,
      title={"Why Should I Trust You?": Explaining the Predictions of Any Classifier}, 
      author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
      year={2016},
      eprint={1602.04938},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{10094635,
  author={Wu, Xiaoliang and Bell, Peter and Rajan, Ajitha},
  booktitle={ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Explanations for Automatic Speech Recognition}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/ICASSP49357.2023.10094635}}

@misc{sun2020explaining,
      title={Explaining Image Classifiers using Statistical Fault Localization}, 
      author={Youcheng Sun and Hana Chockler and Xiaowei Huang and Daniel Kroening},
      year={2020},
      eprint={1908.02374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{unknown,
	author = {Chockler, Hana and Kroening, Daniel and Sun, Youcheng},
	month = {03},
	title = {Compositional Explanations for Image Classifiers},
	year = {2021}
}

@misc{li2017understanding,
      title={Understanding Neural Networks through Representation Erasure}, 
      author={Jiwei Li and Will Monroe and Dan Jurafsky},
      year={2017},
      eprint={1612.08220},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{gandhi2022esb,
      title={ESB: A Benchmark For Multi-Domain End-to-End Speech Recognition}, 
      author={Sanchit Gandhi and Patrick von Platen and Alexander M. Rush},
      year={2022},
      eprint={2210.13352},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@INPROCEEDINGS{7178964,
  author={Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={Librispeech: An ASR corpus based on public domain audio books}, 
  year={2015},
  volume={},
  number={},
  pages={5206-5210},
  doi={10.1109/ICASSP.2015.7178964}}
@misc{ardila2020common,
      title={Common Voice: A Massively-Multilingual Speech Corpus}, 
      author={Rosana Ardila and Megan Branson and Kelly Davis and Michael Henretty and Michael Kohler and Josh Meyer and Reuben Morais and Lindsay Saunders and Francis M. Tyers and Gregor Weber},
      year={2020},
      eprint={1912.06670},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wang2021voxpopuli,
      title={VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation}, 
      author={Changhan Wang and Morgane Rivière and Ann Lee and Anne Wu and Chaitanya Talnikar and Daniel Haziza and Mary Williamson and Juan Pino and Emmanuel Dupoux},
      year={2021},
      eprint={2101.00390},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{rousseau-etal-2012-ted,
    title = "{TED}-{LIUM}: an Automatic Speech Recognition dedicated corpus",
    author = "Rousseau, Anthony  and
      Deléglise, Paul  and
      Estéve, Yannick",
    editor = "Calzolari, Nicoletta  and
      Choukri, Khalid  and
      Declerck, Thierry  and
      Do{\u{g}}an, Mehmet U{\u{g}}ur  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/698_Paper.pdf",
    pages = "125--129",
    abstract = "This paper presents the corpus developed by the LIUM for Automatic Speech Recognition (ASR), based on the TED Talks. This corpus was built during the IWSLT 2011 Evaluation Campaign, and is composed of 118 hours of speech with its accompanying automatically aligned transcripts. We describe the content of the corpus, how the data was collected and processed, how it will be publicly available and how we built an ASR system using this data leading to a WER score of 17.4 {\%}. The official results we obtained at the IWSLT 2011 evaluation campaign are also discussed.",
}
@misc{chen2021gigaspeech,
      title={GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio}, 
      author={Guoguo Chen and Shuzhou Chai and Guanbo Wang and Jiayu Du and Wei-Qiang Zhang and Chao Weng and Dan Su and Daniel Povey and Jan Trmal and Junbo Zhang and Mingjie Jin and Sanjeev Khudanpur and Shinji Watanabe and Shuaijiang Zhao and Wei Zou and Xiangang Li and Xuchen Yao and Yongqing Wang and Yujun Wang and Zhao You and Zhiyong Yan},
      year={2021},
      eprint={2106.06909},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}
@misc{oneill2021spgispeech,
      title={SPGISpeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition}, 
      author={Patrick K. O'Neill and Vitaly Lavrukhin and Somshubra Majumdar and Vahid Noroozi and Yuekai Zhang and Oleksii Kuchaiev and Jagadeesh Balam and Yuliya Dovzhenko and Keenan Freyberg and Michael D. Shulman and Boris Ginsburg and Shinji Watanabe and Georg Kucsko},
      year={2021},
      eprint={2104.02014},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{delrio2022earnings22,
      title={Earnings-22: A Practical Benchmark for Accents in the Wild}, 
      author={Miguel Del Rio and Peter Ha and Quinten McNamara and Corey Miller and Shipra Chandra},
      year={2022},
      eprint={2203.15591},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{Platen_2019,
   title={Multi-Span Acoustic Modelling Using Raw Waveform Signals},
   url={http://dx.doi.org/10.21437/Interspeech.2019-2454},
   DOI={10.21437/interspeech.2019-2454},
   booktitle={Interspeech 2019},
   publisher={ISCA},
   author={Platen, P. von and Zhang, Chao and Woodland, P.C.},
   year={2019},
   month=sep, 
   collection={interspeech_2019} 
}
