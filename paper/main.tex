\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
%\usepackage{cite}
\usepackage{amssymb,amsfonts}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
% Custom personalization
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\parskip 1ex
\parindent 0pt
\usepackage[spanish]{babel}
\usepackage{csquotes}
\usepackage{hyperref}
\usepackage[backend=biber,style=ieee]{biblatex}
\addbibresource{references.bib}
\usepackage{comment}
\usepackage{caption}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    }

\begin{document}

\title{BATS: Bridging Acoustic Transparency in Speech}

\author{\IEEEauthorblockN{Diego Quezada}
\IEEEauthorblockA{\textit{Departamento de Informática} \\
\textit{Universidad Técnica Federico Santa María}\\
Valparaíso, Chile \\
diego.quezadac@sansano.usm.cl}
\and
\IEEEauthorblockN{Felipe Cisternas}
\IEEEauthorblockA{\textit{Departamento de Informática} \\
\textit{Universidad Técnica Federico Santa María}\\
Valparaíso, Chile \\
felipe.cisternasal@sansano.usm.cl}}

\maketitle

\begin{abstract}

El reconocimiento de voz se basa en representaciones de señales acústicas, como espectrogramas y MFCCs. Sin embargo, los modelos actuales son en gran medida opacos en cuanto a cómo toman decisiones en este proceso. La naturaleza física de los datos de entrada en el reconocimiento de voz agrega una capa adicional de complejidad, lo que plantea el desafío de mejorar la transparencia y la comprensión de estos modelos para garantizar un reconocimiento de voz más preciso y confiable.

\end{abstract}

\begin{IEEEkeywords}
ASR, XAI, LIME, Representation Erasure
\end{IEEEkeywords}

\section{Introducción}

En el ámbito del reconocimiento de voz, la selección de un modelo y su arquitectura es fundamental para enfrentar los retos específicos que esta disciplina impone. Modelos de vanguardia como Whisper de OpenAI, que se basan en la arquitectura de Transformers, son altamente eficaces, pero actúan como cajas negras, lo que significa que los procesos de decisión que siguen para convertir señales de audio en palabras no son transparentes. Esta falta de explicabilidad, esencial para generar confianza en los usuarios, hace imperativo el uso de técnicas de eXplainable Artificial Intelligence (XAI) para desentrañar y comprender cómo estos modelos avanzados toman sus decisiones.

La explicabilidad en modelos de reconocimiento de voz no solo mejora la comprensión del proceso de toma de decisiones, sino que también es clave para aumentar su robustez, especialmente cuando dichos modelos se integran como componentes centrales en un software. Esta característica se vuelve esencial para ganar la confianza de los usuarios en el producto final. Sin embargo, el desafío se intensifica debido a la naturaleza física y compleja de los datos de entrada, lo que dificulta proporcionar explicaciones claras y de alto nivel basados en estos datos. Por ello, es crucial mantener un equilibrio entre avanzar en la precisión y eficiencia de los modelos y desarrollar soluciones que mejoren su explicabilidad, asegurando así un balance óptimo entre rendimiento y comprensión del modelo.

A pesar de la importancia de la explicabilidad en el reconocimiento de voz, librerías de explicabilidad ampliamente conocidas, como LIME \cite{ribeiro2016why}, SHAP \cite{lundberg2017unified} y Captum \cite{DBLP:journals/corr/abs-2009-07896}, no ofrecen métodos de explicabilidad para estos modelos. Por ende, es necesario ajustar los métodos existentes para que sean aplicables a esta tarea.

Esta investigación tiene como objetivo mejorar la comprensión de los modelos de reconocimiento de voz para que los usuarios, especialmente aquellos con discapacidades auditivas, puedan confiar en su funcionamiento.

En este estudio, la sección \ref{2-Related-Work} aborda el trabajo previo relacionado con el tema, seguido por una exposición del marco teórico en la sección \ref{3-Theoretical-Framework}. La sección \ref{4-Dataset} detalla el conjunto de datos empleados para la investigación. La metodología adoptada se describe en la sección \ref{5-Methodology}, mientras que los resultados alcanzados se exponen en la sección \ref{6-Results}. Las perspectivas de investigaciones futuras se discuten en la sección \ref{7-Future-Work}. Finalmente, la sección \ref{8-Conclusions} sintetiza las conclusiones derivadas del estudio. Además, la sección \ref{9-Anexos} contiene anexos que ofrecen información adicional y detallada sobre los resultados obtenidos.

\section{Trabajo relacionado} \label{2-Related-Work}

Las investigaciones sobre explicabilidad en modelos de reconocimiento de voz son escasas, pero destacan por su aplicación innovadora de métodos de explicabilidad.

En \cite{10094635} se aborda la tarea reconocimiento de voz utilizando el conjunto de datos CommonVoice \cite{commonvoice:2020} y tres sistemas de reconocimiento de voz: Google API, Sphinx y DeepSpeech. Los autores plantean las explicaciones como un subconjunto de fotogramas de audio que son causas mínimas y suficientes de la transcripción. Para esto, adaptan las técnicas de clasificación de imagen SFL \cite{sun2020explaining} y explicaciones composicionales \cite{unknown} para luego compararlas con las ofrecidas por LIME \cite{ribeiro2016why}.

En \cite{wu2023trust} se aborda la tarea de \textbf{reconocimiento de fonemas} utilizando el conjunto de datos TIMIT \cite{timit} y el método LIME junto a dos variantes propuestas: LIME-WS y LIME-TS. El conjunto de datos TIMIT segmenta los distintos fonemas en los datos de entrada. Los autores utilizan como representación interpretable un vector que indica la presencia o ausencia de cada segmento de audio. El objetivo es identificar los segmentos más importantes para la predicción del siguiente fonema. Inicialmente, para la generación del vecindario se aplican máscaras aleatorias a los segmentos de TIMIT. Luego, considerando que al analizar un fonema es probable que solo los segmentos cercanos incidan en su identificación, LIME-WS solo aplica máscaras aleatorias a segmentos cercanos al fonema analizado y mantiene los lejanos constantes. Finalmente, en LIME-TS se consideran segmentos creados por un intervalo de tiempo en vez de los definidos por TIMIT y se aplica la misma idea de localidad que en LIME-WS.

En \cite{DBLP:journals/corr/abs-2008-00582} se aborda la tarea de etiquetado de música. Los autores plantean la necesidad de dar \textbf{explicaciones audibles} y proponen audioLIME como una extensión de LIME que utiliza explicaciones basadas en separación de fuentes.

\section{Marco Teorico} \label{3-Theoretical-Framework}

\subsection{Speech Recognition}
El reconocimiento de voz, o más conocido en inglés como \textit{speech recognition}, es la tarea de asignar una secuencia de palabras a señales acústicas que contienen lenguaje hablado. 
Implica reconocer las palabras pronunciadas en una grabación de audio y transcribirlas a un formato escrito. El objetivo es transcribir con precisión el discurso en tiempo real o a partir de audio grabado, teniendo en cuenta factores como el acento, la velocidad del habla y el ruido de fondo.
Cuando la transcripción se realiza en tiempo real se habla de reconocimiento automático de voz o \textit{Automatic Speech Recognition} (ASR) en inglés. Considerando $\mathbf{X} = (x^{(1)}, x^{(2)} ,\dots, x^{(T)})$ como una secuencia de audio de largo $T$ e $y = (y_1, y_2, \dots, y_N)$ como una secuencia de palabras de largo $N$ podemos definir la tarea de reconocimiento de voz de manera precisa mediante el siguiente problema de optimización:

\begin{equation}
    f^\ast(\mathbf{X}) = \argmax_{\mathbf{y}} P^\ast(\mathbf{y}| \mathbf{X} = X)
\end{equation}

Donde $P^\ast$ es la verdadera distribución de probabilidad condicional que relaciona las entradas $\mathbf{X}$ con las salidas $\mathbf{y}$ \cite{Goodfellow-et-al-2016}.

La representación utilizada para $\mathbf{X}$ es de vital importancia para el desempeño de un modelo de reconocimiento de voz. La representación más simple es mediante una serie temporal univariada que modela la amplitud de la \textbf{señal de audio} en el tiempo. Al dividir la señal de audio en pequeñas ventanas de tiempo y calculando el espectro de frecuencia para cada ventana se obtiene un \textbf{espectrograma}: una representación visual de la señal de audio en el tiempo y en el dominio de la frecuencia. A partir del espectrograma se pueden extraer características de audio tales como los \textbf{coeficientes cepstrales de Mel} (MFCCs) que son ampliamente utilizados en la literatura para el reconocimiento de voz.

\subsection{Métricas de Evaluación}
Para poder evaluar un modelo de ASR se utilizan métricas como:
\begin{itemize}
    \item \textbf{Word Error Rate (WER):} La métrica Word Error Rate se calcula como el número total de errores, que es la suma de sustituciones, inserciones y eliminaciones de palabras necesarias para cambiar una secuencia de palabras hipotética a una secuencia de referencia, dividido por el número total de palabras en la secuencia de referencia. WER proporciona una medida de cuántas palabras fueron reconocidas incorrectamente en proporción al total de palabras habladas.
    La fórmula para calcular el Word Error Rate (WER) es:
    \begin{equation}
        \text{WER} = \frac{S + D + I}{N}
    \end{equation}
    Donde:
    
    - \( S \) es el número de sustituciones,
    
    - \( D \) es el número de eliminaciones,
    
    - \( I \) es el número de inserciones,
    
    - \( N \) es el número de palabras en la secuencia de referencia.
    
    WER se expresa a menudo como un porcentaje, y cuanto más bajo es el WER, mejor es el rendimiento del sistema de reconocimiento de voz.
    
    \item \textbf{ Match Error Rate (MER):} La métrica Match Error Rate es similar a WER, pero en lugar de centrarse solo en las palabras, evalúa la precisión en el emparejamiento de cualquier tipo de elementos, como fonemas, letras o incluso palabras en tareas de reconocimiento. La fórmula para calcular MER es:
    \begin{equation}
        \text{MER} = \frac{S + D + I}{S + D + C}
    \end{equation}
    Donde:
    
    - \( S \) es el número de sustituciones,
    
    - \( D \) es el número de eliminaciones,
    
    - \( I \) es el número de inserciones,
    
    - \( C \) es el número de aciertos correctos.
    
    MER también se suele expresar en porcentaje y proporciona una medida de cuántos elementos fueron incorrectamente emparejados en relación con el total de elementos que debían emparejarse, cuanto más bajo es MER mejor es el rendimiento del modelo.
    
    \item \textbf{Word Information Lost (WIL):} La métrica Word Information Lost es una medida teórica de la información basada en la entropía desde la perspectiva de la información de las palabras que se pierden en la transcripción de la hipótesis comparada con la referencia. La tasa de WIL se puede calcular con la fórmula:
    \begin{equation}
        \text{WIL} = 1 - \frac{C}{N} + \frac{C}{P}
    \end{equation}
    
    donde:
    
    - \( C \) es el número de palabras correctas,
    
    - \( N \) es el número de palabras en la referencia,
    
    - \( P \) es el número de palabras en la predicción.
    
    La métrica WIL está diseñada para estar acotada entre 0 y 1, proporcionando una forma normalizada de medir la información perdida en la transcripción. Un valor más bajo de WIL indica un mejor rendimiento del sistema de ASR, siendo 0 el puntaje perfecto. Es importante mencionar que, aunque WIL comparte similitudes con WER en cuanto a su uso para evaluar transcripciones de ASR, está diseñada para abordar ciertas limitaciones de WER al proporcionar una medida normalizada que está efectivamente limitada.
    
    \item \textbf{Word Information Preserved (WIP):} La métrica Word Information Preserved mide el porcentaje de palabras correctas predichas entre un conjunto de oraciones verdaderas y un conjunto de oraciones hipotéticas. La fórmula para calcular la tasa de WIP es:
    \begin{equation}
        wip = \frac{C}{N} + \frac{C}{P}
    \end{equation}
    donde:
    
    - \( C \) es el número de palabras correctas,
    
    - \( N \) es el número de palabras en la referencia,
    
    - \( P \) es el número de palabras en la predicción.
    
    Un valor más alto de WIP indica un mejor rendimiento del sistema ASR, siendo 1 la puntuación perfecta.
    \item \textbf{Character Error Rate (CER):} La métrica Character Error Rate se calcula de manera similar a WER, pero en lugar de considerar las palabras, se enfoca en los caracteres individuales. La fórmula para calcular CER es:
    \begin{equation}
        \text{CER} = \frac{S + D + I}{N}
    \end{equation}
    Donde:
    
    - \( S \) es el número de sustituciones de caracteres,
    
    - \( D \) es el número de eliminaciones de caracteres,
    
    - \( I \) es el número de inserciones de caracteres,
    
    - \( N \) es el número total de caracteres en la secuencia de referencia.
    
    Al igual que WER, el CER se expresa en porcentaje, y un valor más bajo indica un mejor rendimiento del sistema de reconocimiento.
\end{itemize}

\subsection{Estado del Arte}
Hoy en día existen variadas arquitecturas de Deep Learning para la tarea de ASR, cada una con sus ventajas y desventajas, es por esto que se utiliza como referencia el \href{https://huggingface.co/spaces/hf-audio/open_asr_leaderboard}{Open ASR Leaderboard} de Hugginface, donde se evalúan diversos modelos frente a distintos datasets. Dado que evaluar un sistema de reconocimiento de voz es una tarea difícil, en el Open ASR Leaderboard se utiliza la estrategia de evaluación multi-dataset propuesta en ESB \cite{gandhi2022esb} para obtener evaluaciones robustas de cada modelo.
ESB es un punto de referencia para evaluar el rendimiento de un único sistema de reconocimiento automático de voz (ASR) en un amplio conjunto de conjuntos de datos de voz. Comprende ocho conjuntos de datos de reconocimiento de voz en inglés, que capturan una amplia gama de dominios, condiciones acústicas, estilos de hablantes y requisitos de transcripción. Como tal, proporciona una mejor indicación de cómo es probable que funcione un modelo en ASR descendente en comparación con evaluarlo en un solo conjunto de datos.
La puntuación ESB se calcula como un macropromedio de las puntuaciones WER en todos los conjuntos de datos de ESB. Los modelos en la tabla de clasificación se clasifican según sus puntuaciones WER promedio, de menor a mayor, y los dataset utilizados son:
\begin{itemize}
    \item \textbf{LibriSpeech:} LibriSpeech \cite{7178964} es un corpus de aproximadamente 1000 horas de voz en inglés leída a 16 kHz. Los datos se derivan de audiolibros leídos del proyecto LibriVox y han sido cuidadosamente segmentados y alineados.
    \item \textbf{Common Voice 9:} Common Voice \cite{ardila2020common} consta de 14973 horas validadas en 93 idiomas junto con sus transcripciones, en el conjunto de datos también incluyen metadatos demográficos como edad, sexo y acento que pueden ayudar a mejorar la precisión de los motores de reconocimiento de voz.
    \item \textbf{VoxPopuli:} VoxPopuli \cite{wang2021voxpopuli} es un corpus de habla multilingüe. Los datos se recopilan de grabaciones de eventos del Parlamento Europeo de 2009-2020. Esta implementación contiene datos de voz transcritos para 18 idiomas. También contiene 29 horas de datos de voz transcritos en inglés no nativo destinado a la investigación en ASR para habla con acento.
    \item \textbf{TED-LIUM:} TED-LIUM \cite{rousseau-etal-2012-ted} son charlas TED en inglés, con transcripciones, muestreadas a 16 kHz. Las tres versiones del corpus oscilan entre 118 y 452 horas de datos de voz transcritos.
    \item \textbf{GigaSpeech:} GigaSpeech \cite{chen2021gigaspeech} es un corpus de reconocimiento de voz en inglés multidominio y en evolución con 10.000 horas de audio etiquetado de alta calidad. Los datos de audio transcritos se recopilan de audiolibros, podcasts y YouTube, y cubren estilos de lectura y de habla espontánea, y una variedad de temas, como artes, ciencias, deportes, etc.
    \item \textbf{SPGISpeech:} SPGISpeech \cite{oneill2021spgispeech} es un corpus de 5000 horas de audio financiero transcrito profesionalmente. SPGISpeech contiene una amplia muestra representativa de acentos en inglés, calidad de audio que varía mucho y habla tanto espontánea como narrada. Cada una de las transcripciones ha sido verificada por varios editores profesionales para garantizar una alta precisión y están completamente formateadas, incluidas las mayúsculas, la puntuación y la desnormalización de palabras no estándar. SPGISpeech consta de 5000 horas de llamadas grabadas sobre resultados de empresas y sus respectivas transcripciones. SPGISpeech contiene aproximadamente 50.000 hablantes. El formato de cada archivo WAV es audio de un solo canal, 16 kHz y 16 bits.
    \item \textbf{Earnings-22:} Earnings 22 \cite{delrio2022earnings22} es un corpus de 119 horas de llamadas de ganancias en inglés recopiladas de empresas globales. El objetivo principal es servir como punto de referencia para los modelos de ASR industriales y académicos en el habla con acento del mundo real.
    \item \textbf{AMI:} AMI Meeting \cite{Platen_2019} es un corpus que consta de 100 horas de grabaciones de reuniones. Las grabaciones utilizan una variedad de señales sincronizadas con una línea de tiempo común. Estos incluyen micrófonos para hablar de cerca y de campo lejano, cámaras de video individuales y de vista de sala, y salida de un proyector de diapositivas y una pizarra electrónica. Durante las reuniones, los participantes también tienen a su disposición bolígrafos no sincronizados que registran lo escrito. Las reuniones se grabaron en inglés utilizando tres salas diferentes con diferentes propiedades acústicas e incluyeron en su mayoría hablantes no nativos.
\end{itemize}

También en ESB incluyen una métrica de latencia, \textbf{Real Time Factor} (RTF). El factor de tiempo real es una medida de la latencia de los sistemas automáticos de reconocimiento de voz, es decir, cuánto tiempo tarda un modelo en procesar una determinada cantidad de voz. Generalmente, se expresa como un múltiplo del tiempo real. Un RTF de 1 significa que procesa la voz tan rápido como se pronuncia, mientras que un RTF de 2 significa que tarda el doble. Por tanto, un valor RTF más bajo indica una latencia más baja.

El modelo escogido para esta investigación fue \textbf{Whisper} de OpenAI, un modelo Open-Source que se encuentra en el top del leaderboard Open ASR.

\subsection{Whisper}
Whisper \cite{radford2022robust}  es un modelo de reconocimiento de voz de propósito general. Está entrenado en un gran conjunto de datos de audio diverso y también es un modelo multitarea que puede realizar reconocimiento de voz multilingüe, traducción de voz e identificación de idioma.

La arquitectura Whisper es un enfoque simple de extremo a extremo, implementado como un Transformer encoder-decoder. El audio de entrada se divide en fragmentos de 30 segundos, se convierte en un espectrograma log-Mel y luego se pasa a un encoder. Se entrena un decoder para predecir el título de texto correspondiente, entremezclado con tokens especiales que dirigen al modelo único a realizar tareas como identificación de idioma, marcas de tiempo a nivel de frase, transcripción de voz multilingüe y traducción de voz al inglés.

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/whisper_1.png}
\caption{Arquitectura Whisper Fuente: OpenAI.} 
\end{figure}

\begin{figure*}[ht]
\centering
\includegraphics[width=1\textwidth]{images/whisper_2.png}
\caption{Tokens MultiTask Training Format, Fuente: OpenAI.}
\end{figure*}

Los modelos Whisper están entrenados para tareas de reconocimiento y traducción de voz, siendo capaces de transcribir audio de voz al texto en el idioma en que se habla (ASR), así como traducirlo al inglés (traducción de voz). Los investigadores de OpenAI desarrollaron modelos para estudiar la solidez de los sistemas de procesamiento del habla entrenados bajo una supervisión débil a gran escala. Existen 7 modelos de diferentes tamaños y capacidades, resumidos en el cuadro \ref{tab:whisper-table}.

\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Tamaño} & \textbf{Parámetros} & \textbf{Solo Inglés} & \textbf{Multilingüe}  \\ \hline
Tiny            & 39 M                & \checkmark           & \checkmark            \\ \hline
Base            & 74 M                & \checkmark           & \checkmark            \\ \hline
Small           & 244 M               & \checkmark           & \checkmark            \\ \hline
Medium          & 769 M               & \checkmark           & \checkmark            \\ \hline
Large           & 1550 M              & x                    & \checkmark            \\ \hline
Large-V2        & 1550 M              & x                    & \checkmark            \\ \hline
Large-V3        & 1550 M              & x                    & \checkmark            \\ \hline   
\end{tabular}
\caption{\label{tab:whisper-table} Modelos Whisper}
\end{center}
\end{table}

Los modelos se entrenan con 680.000 horas de audio y las transcripciones correspondientes recopiladas de Internet. El 65\% de estos datos (o 438.000 horas) representa audio en inglés y sus transcripciones, aproximadamente el 18\% (o 126.000 horas) representa audio en otros idiomas y transcripciones en inglés, mientras que el 17\% final (o 117.000 horas) representa audio en inglés y las transcripciones en el idioma correspondiente. Estos datos no ingleses representan 98 idiomas diferentes.

\subsection{LIME}
LIME \cite{ribeiro2016why} es un modelo agnóstico que explica de manera local las predicciones de un modelo \textbf{caja negra} $f$ mediante la aproximación de $f$ con un modelo \textbf{explicable} $g$ en una vecindad de la instancia de interés $\bm{x}$. LIME utiliza una \textbf{representación interpretable} $\bm{x^\prime} \in \{0,1\}^{d^\prime}$ de la instancia $\bm{x} \in \mathbb{R}^d$. De esta forma, el dominio de la explicación $g$ es $\{0,1\}^{d^\prime}$ y, por lo tanto, $g$ actúa sobre la presencia o ausencia de \textit{componentes interpretables}. Para generar la vecindad se utiliza una función $\pi_x(z)$ que mide la distancia entre dos puntos $x$ y $z$ en el espacio de atributos interpretable. Adicionalmente, dado que la idea es aproximar $f$ mediante un modelo interpretable, se considera una penalización $\Omega$ que mide la complejidad de $g$. Finalmente, y considerando una función de pérdida $\mathcal{L}(f, g, \pi_x)$ que mide la imprecisión del modelo $g$ al aproximar $f$ en la vecindad de $\bm{x}$ de acuerdo a la función de distancia $\pi$, la explicación producida por LIME que asegura \textbf{interpretabilidad} y \textbf{fidelidad local} se obtiene según el siguiente problema de minimización:

\begin{equation}
    \xi(\bm{x}) = \argmin_{g \in G} \mathcal{L}(f, g, \pi_x) + \Omega(g).
\end{equation}

Donde $G$ es el conjunto de todos los modelos interpretables considerados, como por ejemplo modelos lineales o árboles de decisión.

\subsection{Representation Erasure}
El borrado de representaciones \cite{li2017understanding} (Representation Erasure) es un método \textbf{post-hoc} y \textbf{agnóstico al modelo} para entender y explicar las decisiones de un modelo de red neuronal mediante la observación de los efectos que tiene el borrar distintas partes de la representación del modelo. Esto puede incluir dimensiones de vectores de palabras de entrada, unidades ocultas intermedias o palabras de entrada. Se utilizan varias técnicas para analizar los efectos de dicho borrado, como calcular la diferencia relativa en métricas de evaluación o usar aprendizaje por refuerzo para borrar el conjunto mínimo de palabras de entrada con el fin de cambiar la decisión de un modelo neural. Este método ayuda a ofrecer explicaciones claras sobre las decisiones de modelos neuronales y también facilita el análisis de errores en dichos modelos.

\section{Conjunto de datos} \label{4-Dataset}

En la presente investigación se utilizará el conjunto de datos \textit{CommonVoice} \cite{commonvoice:2020}. En particular, se utilizarán las grabaciones en inglés asociadas a la versión 11 de CommonVoice disponible en HuggingFace Datasets.

Los conjunto de entrenamiento, validación y prueba consisten en 948736, 16354 y 16354 grabaciones de audio, que corresponden al 96\%, 2\% y un 2\% de los datos respectivamente. Cada grabación de audio tiene asociada una transcripción. Adicionalmente, cada grabación de audio fue grabada con un \textit{sampling rate} de 48 kHz.

\section{Metodología} \label{5-Methodology}

La propuesta busca evaluar la explicabilidad de modelos del estado del arte en la tarea de reconocimiento de voz mediante métodos \textbf{post-hoc}.

\subsubsection{Borrado de Representaciones}
Adicionalmente, analizaremos los cambios en la salida del modelo Whisper al borrar ciertas partes de la representación de la señal de audio a través de MFCCs.
Sea $f$ un modelo neuronal de ASR, dado un ejemplo $x \in E$ con una transcripción $y$, calcularemos el score de una métrica $M$ con la siguiente fórmula:
\begin{equation}
    S_{M}(x,y) = M(y, f(x))
\end{equation}
Ahora sea $d$ una dimensión de nuestro vector $x$ la cual queremos explorar, denotaremos $S_{M}(x,y,\neg d)$ el score cuando la dimensión $d$ es removida; esto quiere decir que todos sus valores son 0.
La importancia de la dimensión $d$ denotada por $I(d)$ estará dada por la diferencia relativa entre $S_{M}(x,y)$ y $S_{M}(x,y,\neg d)$:
\begin{equation} \label{eq:imp}
    I(d) = \frac{1}{|E|} \sum_{x \in E} \frac{S_{M}(x,y) - S_{M}(x,y,\neg d)}{S_{M}(x,y)}
\end{equation}
Para esto utilizaremos las métricas mencionadas en el marco teórico y calcularemos la importancia de cada dimensión con cada métrica dada la diferencia relativa entre los outputs del modelo.

en la Figura \ref{fig:re-diagram} se presenta un diagrama simplificado del algoritmo:

\begin{figure}[ht]
\centerline{\includegraphics[width=0.5\textwidth]{images/RE-Diagram.png}}
\caption{Diagrama Representation Erasure: Primero se calculan las métricas con todos las dimensiones del espectrograma, después se aplica una máscara representada a través de un vector binario para ir removiendo ciertas dimensiones y se calcula la importancia respecto nuestro vector original.}
\label{fig:re-diagram}
\end{figure}

\subsubsection{SLIME}

Para explicar las transcripciones de Whisper, y tomando inspiración en LIME-TS \cite{wu2023trust}, se propone el método de explicablilidad \textbf{SLIME}.

SLIME fragmenta una grabación de audio en segmentos y considera como representación interpretable un vector booleano que indica la presencia o ausencia de cada segmento. Utilizando esta representación, se genera una vecindad apagando segmentos de manera aleatoria. Cada instancia de la vecindad es transcrita por Whisper, y mediante la distancia de Levenshtein entre la transcripción perturbada y la original se cuantifica el impacto de los silencios, proporcionando una base para una comparación cuantitativa entre las transcripciones. Finalmente, se ajusta un modelo explicable considerando la representación interpretable de los audios como entrada y la distancia de Levenshtein como salida.

Debido a la alta demanda computacional que implica obtener predicciones de Whisper, se limitó la vecindad exploratoria a 100 combinaciones. Esto estableció un equilibrio entre la profundidad del análisis y las restricciones prácticas.

Los coeficientes resultantes de la regresión lineal ofrecen una interpretación directa de la relevancia de cada segmento de audio. Los segmentos asociados con coeficientes negativos son clave; su presencia es vital para mantener la fidelidad de la transcripción original, mientras que su ausencia es indicativa de un incremento en la distancia de Levenshtein, reflejando transcripciones que se desvían de la original. Esta información es útil para discernir las partes del audio que el modelo considera importantes para una transcripción precisa. El siguiente paso es mejorar la segmentación del audio, buscando métodos que puedan captar con mayor precisión las unidades lingüísticas significativas y mejorar la interpretación de las decisiones del modelo.

En el algoritmo \ref{alg:slime} se resume el algoritmo propuesto para obtener las explicaciones de Whisper mediante LIME.

\begin{algorithm}[H]
\caption{SLIME}
\begin{algorithmic}[1]
\State $S \gets \emptyset$
\State $f \gets$ Black-box model
\State $g \gets $ White-box model
\State $n$ $\gets$ Number of segments of 100 ms in audio
\State $x \gets$ Original audio instance
\State $y \gets f(x)$
\State $x' \gets$ Vector of $n$ ones
\State $P \gets$ Generate 100 Binomial perturbations of $x'$
\For{each $p$ in $P$}
    \State $z \gets$ Apply $p$ to $x$
    \State $d \gets$  Levenshtein distance between $y$ and $f(z)$
    \State $z' \gets$ Apply random mask to $x'$
    \State $S \gets S \cup (z', d)$
\EndFor
\State $g \gets$ Fit $g$ to $S$
\State \textbf{end algorithm}
\end{algorithmic}
\label{alg:slime}
\end{algorithm}

Adicionalmente, en la Figura \ref{fig:diagram} se presenta un diagrama simplificado del algoritmo:

\begin{figure}[ht]
\centerline{\includegraphics[width=0.5\textwidth]{./images/diagram.png}}
\caption{SLIME: Funcionamiento}
\label{fig:diagram}
\end{figure}


\section{Resultados} \label{6-Results}

\subsection{Entorno de experimentación}
El entorno de experimentación utilizado para realizar esta investigación fue el siguiente:
\begin{itemize}
    \item CPU: Apple M1 Pro 10 Cores.
    \item GPU: Apple GPU 16 Cores.
    \item RAM: 16G B LPDDR5.
    \item OS: macOS 14.0 Sonoma.
    \item Software: Python 3.10, PyTorch 2.1.0, Transformers 4.35.0, Librosa 0.10.1, Datasets 2.14.6, NumPy 1.25.0, Pandas 2.1.2, NLTK 3.8.1, Scikit-learn 1.3.2, JIWER 3.0.3.
\end{itemize}

Para todos los experimentos se definió una semilla para poder generar resultados reproducibles, la semilla utilizada fue 42.
Como modelo se utilizó whisper-tiny con 39 Millones de parámetros.

\subsection{SLIME}

Consideremos una grabación de audio $x$ cuyo contenido es \textit{it is a busy market town that serves a large surrounding area}. Al aplicar SLIME considerando a esta grabación se obtiene la explicación de la figura \ref{fig:coefficients}.

\begin{figure}[ht]
\centerline{\includegraphics[width=0.5\textwidth]{./images/coefficients.png}}
\caption{SLIME: Coeficientes del modelo explicable}
\label{fig:coefficients}
\end{figure}

Tal como se destacó previamente, la presencia de un coeficiente negativo señala la importancia de un segmento; esto se debe a que su eliminación incrementa la discrepancia de la transcripción respecto a la original, según se mide por la distancia de Levenshtein. Para cuantificar la importancia de cada segmento en una escala de 0 a 1, los coeficientes negativos se proyectan en el eje x y luego se normalizan todos los valores. La figura \ref{fig:importance} ilustra esta normalización, mostrando la importancia asignada a cada segmento.

\begin{figure}[ht]
\centerline{\includegraphics[width=0.5\textwidth]{./images/slime.png}}
\caption{SLIME: Importancias}
\label{fig:importance}
\end{figure}

\subsection{Borrado de representaciones}

En la Figura \ref{fig:cv_imp} podemos observar el cálculo de importancias para distintas métricas evaluadas en el set de test del dataset Common Voice 11, cada celda muestra la importancia de una dimensión (columna) de los MFCCs en cada métrica (fila) para el modelo entrenado.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{images/importance_plot_all_dims.png}
\caption{Mapa de calor de importancias $I(d)$ Normalizadas entre [-1,1] calculado usando la Ecuación \ref{eq:imp}.}
\label{fig:cv_imp}
\end{figure}

Se puede visualizar como la dimensión 4 es la más importante para la métrica WER y CER.
Una importancia mayor implica que al remover esa dimensión las predicciones del modelo empeoran con respecto a la métrica evaluada, por ende es una dimensión importante para el modelo.

\begin{figure*}[ht]
\centering
\includegraphics[width=\textwidth]{images/dim_4_erasured.png}
\caption{Ejemplo: “The new patch is less invasive than the old one, but still causes regressions.” y su espectrograma del MEL al borrar la dimensión 4.}
\label{fig:mel_erasure_dim_4}
\end{figure*}

Al hacer la transformación inversa de los coeficientes de MEL podemos notar como la dimensión 4 equivale a la frecuencia de 153 Hz, esto nos indica que Whisper se basa en las frecuencias más bajas para realizar buenas predicciones.

Para comprender mejor las interacciones del modelo, en cada paso se eliminó la dimensión más importante (Ver Seccion \ref{9-Anexos}), pudimos observar que las 3 dimensiones más importantes son la dimensión 4,8 y 1 que corresponden a 153 Hz, 305 Hz y 38 Hz respectivamente, todas estas bandas de MEL corresponden a frecuencias bajas del espectrograma, lo que nos revela que Whisper se enfoca mayoritariamente en las frecuencias bajas.
Esto puede suceder por varias razones:
\begin{enumerate}
    \item \textbf{Información Fundamental en Frecuencias Bajas:} Las frecuencias bajas en un espectrograma de MEL suelen contener la mayor parte de la energía vocal y son cruciales para entender la voz humana. Estas frecuencias incluyen tonos fundamentales y formantes, que son esenciales para identificar sonidos de vocales y consonantes.
    \item \textbf{Robustez en Ambientes Ruidosos:} Las frecuencias más bajas son menos susceptibles a interferencias de ruido de fondo, lo que hace al modelo más eficiente en entornos ruidosos. Este aspecto es especialmente importante en aplicaciones de la vida real donde el audio no siempre es claro.
    \item \textbf{Mejor Reconocimiento de Voz:} Al centrarse en las bandas bajas, Whisper puede ser más efectivo en reconocer y entender diferentes acentos y modulaciones de voz. Además, estas frecuencias suelen ser más estables y consistentes entre diferentes personas, lo que facilita la generalización del modelo.
   \item \textbf{Eficacia en Datos Comprimidos:} Las frecuencias bajas a menudo se preservan mejor en grabaciones de audio comprimidas, lo que es común en muchos formatos de audio digitales. Al centrarse en estas frecuencias, Whisper puede mantener un alto rendimiento incluso con datos de menor calidad.
   \item \textbf{Identificación de Hablantes:} Las características de las frecuencias bajas pueden ayudar a diferenciar entre distintos hablantes, lo que es útil en situaciones donde hay múltiples personas hablando.
\end{enumerate}

\begin{figure}[H]
\centering
\includegraphics[width=0.5\textwidth]{images/negative_importance.png}
\caption{Mapa de calor de importancias $I(d)$ negativas.}
\label{fig:cv_imp_neg}
\end{figure}

También podemos observar que hay importancias negativas, esto quiere decir que al eliminar esa dimensión las predicciones del modelo mejoran, lo que es un hallazgo muy interesante, ya que podríamos optimizar al modelo borrando ciertas dimensiones.


%\subsection{ORTREL (Optimal Representations Trough Reinforcement Learning)}
%Es por esto que proponemos ORTREL un método de aprendizaje por refuerzo para aprender la representación óptima de los espectrogramas para Whisper, eliminando las representaciones con importancias negativas para aumentar el desempeño del modelo una vez ya está entrenado.


\section{Trabajo Futuro} \label{7-Future-Work}

Mirando hacia el futuro, se está desarrollando un enfoque innovador que involucra el uso de modelos de aprendizaje por refuerzo en el campo de la explicabilidad en modelos de audio de machine learning. Este enfoque se centra en la capacidad del modelo para aprender de manera autónoma a preservar o eliminar aquellas características específicas (dimensiones, segmentos temporales y ambas juntas) que contribuyen al mejoramiento del desempeño de Whisper. Al hacerlo, no solo se espera mejorar la eficiencia y la precisión de los modelos de audio, sino también incrementar su transparencia y comprensibilidad. Este método promete ofrecer una nueva perspectiva en la explicabilidad, permitiendo una introspección más profunda en los procesos de toma de decisiones del modelo y una mejor interpretación de sus resultados. A medida que avanzamos, la integración de estas técnicas de aprendizaje por refuerzo en la explicabilidad de modelos de machine learning abre un emocionante camino para futuras investigaciones y aplicaciones prácticas.

Adicionalmente, se propone la creación la librería \href{https://github.com/diegoquezadac/BATS}{BATS} para la explicación de modelos de reconocimiento de voz, la cual se encuentra en desarrollo y se espera que esté disponible en los próximos meses.

\section{Conclusiones} \label{8-Conclusions}

En el presente estudio, se investigó cómo dotar de explicabilidad a Whisper a través de dos métodos de XAI. Es importante destacar el carácter novedoso de esta investigación, ya que es un primer acercamiento a la explicabilidad en modelos del estado del arte en la tarea de reconocimiento de voz. Adicionalmente, se propuso el algoritmo SLIME: una variante de LIME que permite explicar modelos de reconocimiento de voz.

Por un lado, mediante la técnica de borrado de representaciones se identificó visualmente la importancia de las frecuencias bajas en el desempeño de Whisper. Por el otro, SLIME evidencia los segmentos de audio que son más relevantes para una transcripción precisa. Adicionalmente, SLIME ofrece explicaciones escuchables, lo que permite una interpretación más intuitiva del proceso de toma de decisiones del modelo.

Estos resultados son fundamentales para el diseño de sistemas ASR más precisos y transparentes, esenciales para su aplicación confiable en escenarios del mundo real. Además, el estudio sugiere que la incorporación de estrategias de aprendizaje por refuerzo en la explicabilidad de modelos de audio constituye una vanguardista y prometedora dirección de investigación.

\printbibliography

\section{Anexos} \label{9-Anexos}
\begin{figure*}[h]
\centering
\includegraphics[width=0.75\textwidth]{images/dim_4_erasured.png}
\caption{Espectrograma de potencia de MEL al eliminar la banda 4, equivalente a 153 Hz}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.75\textwidth]{images/dim_8_erasured.png}
\caption{Espectrograma de potencia de MEL al eliminar la banda 8, equivalente a 305 Hz}
\end{figure*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.75\textwidth]{images/dim_1_erasured.png}
\caption{Espectrograma de potencia de MEL al eliminar la banda 1, equivalente a 38 Hz}
\end{figure*}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/importance_plot_no_dims_4.png}
\caption{Mapa de Importancias al eliminar la dimensión 4 de los MFCC}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/importance_plot_no_dims_4-8.png}
\caption{Mapa de Importancias al eliminar la dimensión 4 y 8 de los MFCC}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/importance_plot_no_dims_4-8-1.png}
\caption{Mapa de Importancias al eliminar la dimensión 4, 8 y 1 de los MFCC}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{images/importance_plot_no_dims_4-8-1-58.png}
\caption{Mapa de Importancias al eliminar la dimensión 4, 8, 1, 58 de los MFCC}
\end{figure}


\end{document}