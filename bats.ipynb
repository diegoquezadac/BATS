{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset, Audio\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from transformers import AutoProcessor, AutoModelForCTC, TrainingArguments, Trainer, pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataset import CommonVoice\n",
    "from src.data.vocab import SimpleVocab\n",
    "from src.training.early_stopping import CustomEarlyStopping\n",
    "from src.training.train import train_model\n",
    "from src.training.evaluate import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fcisternas/miniconda3/envs/bats/lib/python3.10/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "ds_train = CommonVoice(\"train\")\n",
    "ds_val = CommonVoice(\"validation\")\n",
    "ds_test = CommonVoice(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 200)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data for training\n",
    "def collate_fn(batch):\n",
    "    x, y = zip(*batch)\n",
    "    x = nn.utils.rnn.pad_sequence(x, batch_first=True)\n",
    "    y = nn.utils.rnn.pad_sequence(y, batch_first=True)\n",
    "    return x, y\n",
    "\n",
    "batch_size = 32\n",
    "dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "dl_val = DataLoader(ds_val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "#create pytorch model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/fcisternas/Desktop/BATS/bats.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y240sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m dl_train:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y240sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y240sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(y\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/bats/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/bats/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/bats/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32m/Users/fcisternas/Desktop/BATS/bats.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y240sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollate_fn\u001b[39m(batch):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y240sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     x, y \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y240sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mrnn\u001b[39m.\u001b[39;49mpad_sequence(x, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y240sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     y \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpad_sequence(y, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y240sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x, y\n",
      "File \u001b[0;32m~/miniconda3/envs/bats/lib/python3.10/site-packages/torch/nn/utils/rnn.py:400\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    396\u001b[0m         sequences \u001b[39m=\u001b[39m sequences\u001b[39m.\u001b[39munbind(\u001b[39m0\u001b[39m)\n\u001b[1;32m    398\u001b[0m \u001b[39m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mpad_sequence(sequences, batch_first, padding_value)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "for x,y in dl_train:\n",
    "    print(x.shape)\n",
    "    print(y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pytorch model\n",
    "\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size=128, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=40,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.linear = nn.Linear(2 * hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model = BaselineModel(output_size=y.shape[0])\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "early_stopping = CustomEarlyStopping(patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traning loop from scratch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparametros del modelo\n",
    "input_size = x.shape[1]\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "dropout = 0.2\n",
    "bidirectional = True\n",
    "layer_tipe = nn.LSTM\n",
    "output_size = y.shape[0]\n",
    "batch_size = x.shape[0]\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "\n",
    "#custom early stopping\n",
    "early_stopping = CustomEarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "# Para RNN simple\n",
    "model = RNNModel(input_size, hidden_size, output_size,num_layers, dropout, bidirectional, layer_tipe)\n",
    "\n",
    "#optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.training.early_stopping import CustomEarlyStopping\n",
    "\n",
    "def train_model(model: torch.nn.Module, train_loader: torch.utils.data.DataLoader, val_loader: torch.utils.data.DataLoader, epochs: int, criterion: torch.nn, optimizer: torch.optim, early_stopping: CustomEarlyStopping = None):\n",
    "    \"\"\"\n",
    "    Trains a PyTorch model with the given data loaders, criterion, and optimizer.\n",
    "\n",
    "    Args:\n",
    "        model: The PyTorch model to be trained.\n",
    "        train_loader: DataLoader for the training data.\n",
    "        val_loader: DataLoader for the validation data.\n",
    "        epochs: The number of epochs to train the model.\n",
    "        criterion: The loss function used to evaluate the model's performance.\n",
    "        optimizer: The optimization algorithm used to update the model's weights.\n",
    "        device: The device (CPU or GPU) to train the model on.\n",
    "        early_stopping: An optional instance of CustomEarlyStopping to stop training early if validation loss does not improve.\n",
    "\n",
    "    Returns:\n",
    "        The trained model with the best validation loss if early stopping is used, otherwise the model after the last training epoch.\n",
    "    \"\"\"\n",
    "    # Iterate over the entire dataset for a specified number of epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Set the model to training mode\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Iterate over the training data\n",
    "        for inputs, labels in train_loader:\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass: compute the model output\n",
    "            outputs = model(inputs)\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass: compute the gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # Perform a single optimization step to update the model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate the training loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Set the model to evaluation mode\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        # Disable gradient calculation for validation to save memory and computations\n",
    "        with torch.no_grad():\n",
    "            # Iterate over the validation data\n",
    "            for inputs, labels in val_loader:\n",
    "                # Compute the model output\n",
    "                outputs = model(inputs)\n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Accumulate the validation loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Print the training and validation loss statistics\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "\n",
    "        # Early Stopping check\n",
    "        if early_stopping is not None:\n",
    "            # Call the early stopping logic\n",
    "            early_stopping(val_loss/len(val_loader), model)\n",
    "            if early_stopping.early_stop:\n",
    "                # If early stop condition was met, print a message and break the training loop\n",
    "                print(\"Early stopping\")\n",
    "                # Load the best model state (with the lowest validation loss)\n",
    "                model.load_state_dict(early_stopping.best_state)\n",
    "                break\n",
    "\n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/fcisternas/Desktop/BATS/bats.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m train_model(model, dl_train, dl_val, epochs, criterion, optimizer, early_stopping)\n",
      "\u001b[1;32m/Users/fcisternas/Desktop/BATS/bats.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Iterate over the training data\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39m# Zero the parameter gradients\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \u001b[39m# Forward pass: compute the model output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bats/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/bats/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/bats/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32m/Users/fcisternas/Desktop/BATS/bats.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollate_fn\u001b[39m(batch):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     x, y \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     x \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mrnn\u001b[39m.\u001b[39;49mpad_sequence(x, batch_first\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     y \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mrnn\u001b[39m.\u001b[39mpad_sequence(y, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/fcisternas/Desktop/BATS/bats.ipynb#Y225sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x, y\n",
      "File \u001b[0;32m~/miniconda3/envs/bats/lib/python3.10/site-packages/torch/nn/utils/rnn.py:400\u001b[0m, in \u001b[0;36mpad_sequence\u001b[0;34m(sequences, batch_first, padding_value)\u001b[0m\n\u001b[1;32m    396\u001b[0m         sequences \u001b[39m=\u001b[39m sequences\u001b[39m.\u001b[39munbind(\u001b[39m0\u001b[39m)\n\u001b[1;32m    398\u001b[0m \u001b[39m# assuming trailing dimensions and type of all the Tensors\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39m# in sequences are same and fetching those from sequences[0]\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mpad_sequence(sequences, batch_first, padding_value)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "model = train_model(model, dl_train, dl_val, epochs, criterion, optimizer, early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout, bidirectional, rnn_class):\n",
    "        super().__init__()\n",
    "        # Verificar que rnn_class es una de las clases de RNN permitidas\n",
    "        if rnn_class not in [nn.LSTM, nn.GRU, nn.RNN]:\n",
    "            raise ValueError(\"rnn_class must be nn.LSTM, nn.GRU, or nn.RNN\")\n",
    "\n",
    "        # Inicializar la capa recurrente con la clase pasada como parámetro\n",
    "        self.rnn = rnn_class(input_size=input_size,\n",
    "                             hidden_size=hidden_size,\n",
    "                             num_layers=num_layers,\n",
    "                             batch_first=True,\n",
    "                             bidirectional=bidirectional)\n",
    "        \n",
    "        # Calcular la dimensión de salida de la capa recurrente\n",
    "        # Es doble si es bidireccional\n",
    "        factor = 2 if bidirectional else 1\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear1 = nn.Linear(hidden_size * factor, 64)\n",
    "        self.linear2 = nn.Linear(64, 32)\n",
    "        self.output_linear = nn.Linear(32, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM devuelve (output, (h_n, c_n)) mientras que GRU y RNN solo devuelve (output, h_n)\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.output_linear(x)\n",
    "        return x\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bats",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
